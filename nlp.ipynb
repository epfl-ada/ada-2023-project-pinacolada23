{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677da2d5-0214-4139-92e3-05d5c683e12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# External imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95be96d-586f-4a10-9422-e879a35a7e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r character_movies_filtered_imdb\n",
    "df_prepared_copie = character_movies_filtered_imdb.copy()\n",
    "df_prepared_copie = df_prepared_copie[['wikiID','releaseDate','charactName','name_movie']]\n",
    "\n",
    "liste_wikiID_uniques = df_prepared_copie['wikiID'].unique()\n",
    "# display(df_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a9a54-efc5-4ae1-8770-d90c835bc185",
   "metadata": {},
   "source": [
    "## Extraction only of the summaries of interest to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50a86b19-ce11-44fe-9872-8a8835bfba84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5964/5964 [00:08<00:00, 662.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Home directory\n",
    "source_directory = 'corenlp_plot_summaries'\n",
    "\n",
    "# Destination directory for files corresponding to wikiIDs\n",
    "destination_directory = 'MovieSummaries/tried_summaries'\n",
    "\n",
    "# Directory creation\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.makedirs(destination_directory)\n",
    "\n",
    "# Get a list of files in the source directory\n",
    "source_files = os.listdir(source_directory)\n",
    "\n",
    "# Loop through the files in the source directory\n",
    "for files in tqdm(source_files):\n",
    "    # Checks if the file matches a wikiID in the list\n",
    "    wikiID = files.replace('.xml.gz', '')\n",
    "    if int(wikiID) in liste_wikiID_uniques:\n",
    "        # Build complete source and destination file paths\n",
    "        source_path = os.path.join(source_directory, files)\n",
    "        destination_path = os.path.join(destination_directory, f\"{wikiID}.txt.xml\")\n",
    "\n",
    "        # Unzip the gzip file\n",
    "        with gzip.open(source_path, 'rt', encoding='utf-8') as f_in:\n",
    "            # Copy the contents to a .txt.xml file in the new directory\n",
    "            with open(destination_path, 'w', encoding='utf-8') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Extraction complete.\")\n",
    "# print(len(os.listdir(destination_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e92c8ac0-7b32-4356-825b-7e3de6d9637c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5707\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(destination_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7dd4c-6107-4e79-9410-78984443474a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/xenia/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      " 97%|█████████████████████████████████████▉ | 5845/6018 [04:59<00:12, 13.67it/s]"
     ]
    }
   ],
   "source": [
    "repertoire= 'MovieSummaries/tried_summaries'\n",
    "dependencies = [\"nsubj\", \"agent\", \"dobj\", \"nsubjpass\", \"iobj\", \"prep_*\"]\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "for wikiID in tqdm(liste_wikiID_uniques[136:]):\n",
    "    document = os.path.join(repertoire, str(wikiID) + '.txt.xml')\n",
    "    if os.path.exists(document):\n",
    "        # Access to XML file\n",
    "        tree = ET.parse(document)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Find the film's characters\n",
    "        df_movie = df_prepared_copie[df_prepared_copie['wikiID']==wikiID]\n",
    "        df_movie = df_movie.dropna(subset=['charactName'])\n",
    "        characters_names = df_movie['charactName'].tolist()\n",
    "\n",
    "        ##### Finding the characters' names ie entities that have a PERSON label. ##### \n",
    "        # Lists to store sentence IDs, token IDs, and words\n",
    "        sentences_id_PER = []\n",
    "        tokens_id_PER = []\n",
    "        words_PER = []\n",
    "        \n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        prev_sentence_id = None\n",
    "        prev_token_id = None\n",
    "        \n",
    "        \n",
    "        # Retrieve each sentence from the summary\n",
    "        for sentence in root.findall('.//sentence'):\n",
    "            sentence_id = sentence.attrib.get('id')\n",
    "            # Retrieve each word in the sentence\n",
    "            for token in sentence.findall('.//token'):\n",
    "                # Extract PER entities\n",
    "                ner = token.find('NER').text\n",
    "                if ner == \"PERSON\":\n",
    "                    token_id = int(token.get('id'))\n",
    "                    word = token.find('word').text\n",
    "                    \n",
    "                    # Group words into combined entities \n",
    "                    if prev_sentence_id != sentence_id or prev_token_id is None or token_id != prev_token_id + 1:\n",
    "                        if current_entity:\n",
    "                            entities.append(' '.join(current_entity))\n",
    "                            current_entity = []\n",
    "                    current_entity.append(word)\n",
    "                    prev_sentence_id = sentence_id\n",
    "                    prev_token_id = token_id\n",
    "                    \n",
    "        # Add the last entity if it exists\n",
    "        if current_entity:\n",
    "            entities.append(' '.join(current_entity))            \n",
    "\n",
    "        normalized_entities = {}\n",
    "        \n",
    "        if len(entities) != 0:\n",
    "            for name in entities:\n",
    "                #Normalize each entity so that it is the same as the datafream entity\n",
    "                normalized_name = next((char_name for char_name in characters_names if name in char_name), name)\n",
    "                \n",
    "                # Count their number of recurrences\n",
    "                if normalized_name in normalized_entities:\n",
    "                    normalized_entities[normalized_name] += 1\n",
    "                else:\n",
    "                    normalized_entities[normalized_name] = 1\n",
    "                    \n",
    "            # Calculate the total number of appearances\n",
    "            total_appearances = sum(normalized_entities.values())\n",
    "\n",
    "            # Calculate the percentage of appearances for each name\n",
    "            appearance_percentages = {name: round((count / total_appearances) * 100, 2) for name, count in normalized_entities.items()}\n",
    "            \n",
    "            # Calculate the maximal number of appearances\n",
    "            maximal_value = max(normalized_entities.values())\n",
    "            \n",
    "            # Calculate score of appearances for each name\n",
    "            appearance_importance = {name: round((count / maximal_value), 2) for name, count in normalized_entities.items()}\n",
    "                \n",
    "        \n",
    "        \n",
    "        ##### Finding the polarity_scores for each name. ##### \n",
    "        active_actions = {}\n",
    "        passive_actions = {}\n",
    "        attributes = {}\n",
    "        \n",
    "        # Retrieve each sentence from the summary\n",
    "        for sentence in root.findall('.//sentence'):\n",
    "            sentence_id = sentence.attrib.get('id')\n",
    "                \n",
    "            for dep in sentence.findall('.//collapsed-ccprocessed-dependencies/dep'):\n",
    "                all_words = sentence.findall('.//token')\n",
    "\n",
    "                # Get information on dependencies\n",
    "                dep_type = dep.get('type')\n",
    "                governor = dep.find('governor').text\n",
    "                dependent = dep.find('dependent').text\n",
    "                dependent_id = int(dep.find('dependent').get('idx'))\n",
    "                governor_id = int(dep.find('governor').get('idx'))\n",
    "\n",
    "                # Get information on governor\n",
    "                word = all_words[governor_id-1].find('word').text\n",
    "                pos_tag = all_words[governor_id-1].find('POS').text\n",
    "                \n",
    "                if word != governor:\n",
    "                    print('error!!')\n",
    "                    \n",
    "                # Find verbs with particular dependencies\n",
    "                if pos_tag.startswith('V'):\n",
    "                    if dep_type in dependencies:\n",
    "                        # Find out if the verb is an action of a character\n",
    "                        for name in characters_names:\n",
    "                            if (dep_type == \"nsubj\" or dep_type == \"agent\") and (governor == word) and (dependent in name):\n",
    "                                if name not in active_actions:\n",
    "                                    active_actions[name] = []\n",
    "                                active_actions[name].append(word)\n",
    "                                \n",
    "                            # Find out if the verb is an action on a character\n",
    "                            elif (dep_type in [\"dobj\", \"nsubjpass\", \"iobj\", \"prep_*\"]) and (governor == word) and (dependent in name):\n",
    "                                if name not in passive_actions:\n",
    "                                    passive_actions[name] = []\n",
    "                                passive_actions[name].append(word)\n",
    "\n",
    "                # Find the word that describes a character\n",
    "                if (pos_tag == \"JJ\" or pos_tag == \"VBG\" or pos_tag == \"NN\"):\n",
    "                    for name in characters_names:#itérer sur tout les perso\n",
    "                        if ((dep_type == \"nsubj\" or dep_type == \"appos\") and (governor == word) and (dependent in name)):\n",
    "                            if name not in attributes:\n",
    "                                attributes[name] = []\n",
    "                            attributes[name].append(word)\n",
    "                \n",
    "                # Get information on dependent\n",
    "                word = all_words[dependent_id-1].find('word').text\n",
    "                pos_tag = all_words[dependent_id-1].find('POS').text\n",
    "                # Find the word that describes a character\n",
    "                for name in characters_names:\n",
    "                    if (dep_type == \"nsubj\" or dep_type == \"amod\" or dep_type == \"nn\" or dep_type == \"appos\") and (governor in name) and (dependent == word):\n",
    "                        if name not in attributes:\n",
    "                            attributes[name] = []\n",
    "                        attributes[name].append(word)\n",
    "        \n",
    "        # Complete fream data\n",
    "        for index, row in df_movie.iterrows():\n",
    "            df_prepared_copie.loc[index, 'role_summary_percent'] = appearance_percentages.get(row['charactName'], 0)\n",
    "            df_prepared_copie.loc[index, 'role_importance'] = appearance_importance.get(row['charactName'], 0)\n",
    "           \n",
    "            actif_v = \" \".join(active_actions.get(row['charactName'], []))\n",
    "            pasif_v = \" \".join(passive_actions.get(row['charactName'], []))\n",
    "            adj = \" \".join(attributes.get(row['charactName'], []))\n",
    "    \n",
    "            df_prepared_copie.loc[index, 'comp_active'] = sia.polarity_scores(actif_v)[\"compound\"]\n",
    "            df_prepared_copie.loc[index, 'comp_pasive'] = sia.polarity_scores(pasif_v)[\"compound\"]\n",
    "            df_prepared_copie.loc[index, 'comp_attribut'] = sia.polarity_scores(adj)[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090c61e-356d-436e-b080-c5338d4e0b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prepared_copie[df_prepared['wikiID']==31186339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608aa70-0816-4e68-bbab-c660c68b0ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prepared_copie.to_csv('caracter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534c69c-8a09-42eb-a5e3-f2bdc3a490d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

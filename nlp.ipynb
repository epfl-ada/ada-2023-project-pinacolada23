{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "677da2d5-0214-4139-92e3-05d5c683e12b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# External imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c95be96d-586f-4a10-9422-e879a35a7e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r character_movies_filtered_imdb\n",
    "df_prepared_copie = character_movies_filtered_imdb.copy()\n",
    "df_prepared_copie = df_prepared_copie[['wikiID','releaseDate','charactName','name_actor','name_movie']]\n",
    "\n",
    "liste_wikiID_uniques = df_prepared_copie['wikiID'].unique()\n",
    "# display(df_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a9a54-efc5-4ae1-8770-d90c835bc185",
   "metadata": {},
   "source": [
    "## Extraction only of the summaries of interest to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a86b19-ce11-44fe-9872-8a8835bfba84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5964/5964 [00:21<00:00, 280.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Home directory\n",
    "source_directory = 'corenlp_plot_summaries'\n",
    "\n",
    "# Destination directory for files corresponding to wikiIDs\n",
    "destination_directory = 'MovieSummaries/tried_summaries'\n",
    "\n",
    "# Directory creation\n",
    "if not os.path.exists(destination_directory):\n",
    "    os.makedirs(destination_directory)\n",
    "\n",
    "# Get a list of files in the source directory\n",
    "source_files = os.listdir(source_directory)\n",
    "\n",
    "# Loop through the files in the source directory\n",
    "for files in tqdm(source_files):\n",
    "    # Checks if the file matches a wikiID in the list\n",
    "    wikiID = files.replace('.xml.gz', '')\n",
    "    if int(wikiID) in liste_wikiID_uniques:\n",
    "        # Build complete source and destination file paths\n",
    "        source_path = os.path.join(source_directory, files)\n",
    "        destination_path = os.path.join(destination_directory, f\"{wikiID}.txt.xml\")\n",
    "\n",
    "        # Unzip the gzip file\n",
    "        with gzip.open(source_path, 'rt', encoding='utf-8') as f_in:\n",
    "            # Copy the contents to a .txt.xml file in the new directory\n",
    "            with open(destination_path, 'w', encoding='utf-8') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "print(\"Extraction complete.\")\n",
    "# print(len(os.listdir(destination_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92c8ac0-7b32-4356-825b-7e3de6d9637c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5707\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(destination_directory)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d7dd4c-6107-4e79-9410-78984443474a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/xenia/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "100%|███████████████████████████████████████| 6018/6018 [04:49<00:00, 20.78it/s]\n"
     ]
    }
   ],
   "source": [
    "repertoire= 'MovieSummaries/tried_summaries'\n",
    "dependencies = [\"nsubj\", \"agent\", \"dobj\", \"nsubjpass\", \"iobj\", \"prep_*\"]\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "for wikiID in tqdm(liste_wikiID_uniques[136:]):\n",
    "    document = os.path.join(repertoire, str(wikiID) + '.txt.xml')\n",
    "    if os.path.exists(document):\n",
    "        # Access to XML file\n",
    "        tree = ET.parse(document)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Find the film's characters\n",
    "        df_movie = df_prepared_copie[df_prepared_copie['wikiID']==wikiID]\n",
    "        df_movie = df_movie.dropna(subset=['charactName'])\n",
    "        characters_names = df_movie['charactName'].tolist()\n",
    "\n",
    "        ##### Finding the characters' names ie entities that have a PERSON label. ##### \n",
    "        # Lists to store sentence IDs, token IDs, and words\n",
    "        sentences_id_PER = []\n",
    "        tokens_id_PER = []\n",
    "        words_PER = []\n",
    "        \n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        prev_sentence_id = None\n",
    "        prev_token_id = None\n",
    "        \n",
    "        \n",
    "        # Retrieve each sentence from the summary\n",
    "        for sentence in root.findall('.//sentence'):\n",
    "            sentence_id = sentence.attrib.get('id')\n",
    "            # Retrieve each word in the sentence\n",
    "            for token in sentence.findall('.//token'):\n",
    "                # Extract PER entities\n",
    "                ner = token.find('NER').text\n",
    "                if ner == \"PERSON\":\n",
    "                    token_id = int(token.get('id'))\n",
    "                    word = token.find('word').text\n",
    "                    \n",
    "                    # Group words into combined entities \n",
    "                    if prev_sentence_id != sentence_id or prev_token_id is None or token_id != prev_token_id + 1:\n",
    "                        if current_entity:\n",
    "                            entities.append(' '.join(current_entity))\n",
    "                            current_entity = []\n",
    "                    current_entity.append(word)\n",
    "                    prev_sentence_id = sentence_id\n",
    "                    prev_token_id = token_id\n",
    "                    \n",
    "        # Add the last entity if it exists\n",
    "        if current_entity:\n",
    "            entities.append(' '.join(current_entity))            \n",
    "\n",
    "        normalized_entities = {}\n",
    "        \n",
    "        if len(entities) != 0:\n",
    "            for name in entities:\n",
    "                #Normalize each entity so that it is the same as the datafream entity\n",
    "                normalized_name = next((char_name for char_name in characters_names if name in char_name), name)\n",
    "                \n",
    "                # Count their number of recurrences\n",
    "                if normalized_name in normalized_entities:\n",
    "                    normalized_entities[normalized_name] += 1\n",
    "                else:\n",
    "                    normalized_entities[normalized_name] = 1\n",
    "                    \n",
    "            # Calculate the total number of appearances\n",
    "            total_appearances = sum(normalized_entities.values())\n",
    "\n",
    "            # Calculate the percentage of appearances for each name\n",
    "            appearance_percentages = {name: round((count / total_appearances) * 100, 2) for name, count in normalized_entities.items()}\n",
    "            \n",
    "            # Calculate the maximal number of appearances\n",
    "            maximal_value = max(normalized_entities.values())\n",
    "            \n",
    "            # Calculate score of appearances for each name\n",
    "            appearance_importance = {name: round((count / maximal_value), 2) for name, count in normalized_entities.items()}\n",
    "                \n",
    "        \n",
    "        \n",
    "        ##### Finding the polarity_scores for each name. ##### \n",
    "        active_actions = {}\n",
    "        passive_actions = {}\n",
    "        attributes = {}\n",
    "        \n",
    "        # Retrieve each sentence from the summary\n",
    "        for sentence in root.findall('.//sentence'):\n",
    "            sentence_id = sentence.attrib.get('id')\n",
    "                \n",
    "            for dep in sentence.findall('.//collapsed-ccprocessed-dependencies/dep'):\n",
    "                all_words = sentence.findall('.//token')\n",
    "\n",
    "                # Get information on dependencies\n",
    "                dep_type = dep.get('type')\n",
    "                governor = dep.find('governor').text\n",
    "                dependent = dep.find('dependent').text\n",
    "                dependent_id = int(dep.find('dependent').get('idx'))\n",
    "                governor_id = int(dep.find('governor').get('idx'))\n",
    "\n",
    "                # Get information on governor\n",
    "                word = all_words[governor_id-1].find('word').text\n",
    "                pos_tag = all_words[governor_id-1].find('POS').text\n",
    "                \n",
    "                if word != governor:\n",
    "                    print('error!!')\n",
    "                    \n",
    "                # Find verbs with particular dependencies\n",
    "                if pos_tag.startswith('V'):\n",
    "                    if dep_type in dependencies:\n",
    "                        # Find out if the verb is an action of a character\n",
    "                        for name in characters_names:\n",
    "                            if (dep_type == \"nsubj\" or dep_type == \"agent\") and (governor == word) and (dependent in name):\n",
    "                                if name not in active_actions:\n",
    "                                    active_actions[name] = []\n",
    "                                active_actions[name].append(word)\n",
    "                                \n",
    "                            # Find out if the verb is an action on a character\n",
    "                            elif (dep_type in [\"dobj\", \"nsubjpass\", \"iobj\", \"prep_*\"]) and (governor == word) and (dependent in name):\n",
    "                                if name not in passive_actions:\n",
    "                                    passive_actions[name] = []\n",
    "                                passive_actions[name].append(word)\n",
    "\n",
    "                # Find the word that describes a character\n",
    "                if (pos_tag == \"JJ\" or pos_tag == \"VBG\" or pos_tag == \"NN\"):\n",
    "                    for name in characters_names:#itérer sur tout les perso\n",
    "                        if ((dep_type == \"nsubj\" or dep_type == \"appos\") and (governor == word) and (dependent in name)):\n",
    "                            if name not in attributes:\n",
    "                                attributes[name] = []\n",
    "                            attributes[name].append(word)\n",
    "                \n",
    "                # Get information on dependent\n",
    "                word = all_words[dependent_id-1].find('word').text\n",
    "                pos_tag = all_words[dependent_id-1].find('POS').text\n",
    "                # Find the word that describes a character\n",
    "                for name in characters_names:\n",
    "                    if (dep_type == \"nsubj\" or dep_type == \"amod\" or dep_type == \"nn\" or dep_type == \"appos\") and (governor in name) and (dependent == word):\n",
    "                        if name not in attributes:\n",
    "                            attributes[name] = []\n",
    "                        attributes[name].append(word)\n",
    "        \n",
    "        # Complete fream data\n",
    "        for index, row in df_movie.iterrows():\n",
    "            df_prepared_copie.loc[index, 'role_summary_percent'] = appearance_percentages.get(row['charactName'], 0)\n",
    "            df_prepared_copie.loc[index, 'role_importance'] = appearance_importance.get(row['charactName'], 0)\n",
    "           \n",
    "            actif_v = \" \".join(active_actions.get(row['charactName'], []))\n",
    "            pasif_v = \" \".join(passive_actions.get(row['charactName'], []))\n",
    "            adj = \" \".join(attributes.get(row['charactName'], []))\n",
    "    \n",
    "            df_prepared_copie.loc[index, 'comp_active'] = sia.polarity_scores(actif_v)[\"compound\"]\n",
    "            df_prepared_copie.loc[index, 'comp_pasive'] = sia.polarity_scores(pasif_v)[\"compound\"]\n",
    "            df_prepared_copie.loc[index, 'comp_attribut'] = sia.polarity_scores(adj)[\"compound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9090c61e-356d-436e-b080-c5338d4e0b91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wikiID</th>\n",
       "      <th>releaseDate</th>\n",
       "      <th>charactName</th>\n",
       "      <th>name_actor</th>\n",
       "      <th>name_movie</th>\n",
       "      <th>role_summary_percent</th>\n",
       "      <th>role_importance</th>\n",
       "      <th>comp_active</th>\n",
       "      <th>comp_pasive</th>\n",
       "      <th>comp_attribut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50907</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Katniss Everdeen</td>\n",
       "      <td>Jennifer Lawrence</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>51.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50908</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Peeta Mellark</td>\n",
       "      <td>Josh Hutcherson</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>22.86</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50909</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Effie Trinket</td>\n",
       "      <td>Elizabeth Banks</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50910</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Gale Hawthorne</td>\n",
       "      <td>Liam Hemsworth</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50911</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Haymitch Abernathy</td>\n",
       "      <td>Woody Harrelson</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>11.43</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50912</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Clove</td>\n",
       "      <td>Isabelle Fuhrman</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50913</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Caesar Flickerman</td>\n",
       "      <td>Stanley Tucci</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50914</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>President Snow</td>\n",
       "      <td>Donald Sutherland</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50915</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Cato</td>\n",
       "      <td>Alexander Ludwig</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.5423</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50916</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Cinna</td>\n",
       "      <td>Lenny Kravitz</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50917</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Seneca Crane</td>\n",
       "      <td>Wes Bentley</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50918</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Rue</td>\n",
       "      <td>Amandla Stenberg</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>-0.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50919</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Glimmer</td>\n",
       "      <td>Leven Rambin</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50920</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Claudius Templesmith</td>\n",
       "      <td>Toby Jones</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50921</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Mrs. Everdeen</td>\n",
       "      <td>Paula Malcomson</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50922</th>\n",
       "      <td>31186339</td>\n",
       "      <td>2012-03-12</td>\n",
       "      <td>Game Tech</td>\n",
       "      <td>Eric Hennig</td>\n",
       "      <td>The Hunger Games</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         wikiID releaseDate           charactName         name_actor  \\\n",
       "50907  31186339  2012-03-12      Katniss Everdeen  Jennifer Lawrence   \n",
       "50908  31186339  2012-03-12         Peeta Mellark    Josh Hutcherson   \n",
       "50909  31186339  2012-03-12         Effie Trinket    Elizabeth Banks   \n",
       "50910  31186339  2012-03-12        Gale Hawthorne     Liam Hemsworth   \n",
       "50911  31186339  2012-03-12    Haymitch Abernathy    Woody Harrelson   \n",
       "50912  31186339  2012-03-12                 Clove   Isabelle Fuhrman   \n",
       "50913  31186339  2012-03-12     Caesar Flickerman      Stanley Tucci   \n",
       "50914  31186339  2012-03-12        President Snow  Donald Sutherland   \n",
       "50915  31186339  2012-03-12                  Cato   Alexander Ludwig   \n",
       "50916  31186339  2012-03-12                 Cinna      Lenny Kravitz   \n",
       "50917  31186339  2012-03-12          Seneca Crane        Wes Bentley   \n",
       "50918  31186339  2012-03-12                   Rue   Amandla Stenberg   \n",
       "50919  31186339  2012-03-12               Glimmer       Leven Rambin   \n",
       "50920  31186339  2012-03-12  Claudius Templesmith         Toby Jones   \n",
       "50921  31186339  2012-03-12         Mrs. Everdeen    Paula Malcomson   \n",
       "50922  31186339  2012-03-12             Game Tech        Eric Hennig   \n",
       "\n",
       "             name_movie  role_summary_percent  role_importance  comp_active  \\\n",
       "50907  The Hunger Games                 51.43             1.00      -0.3400   \n",
       "50908  The Hunger Games                 22.86             0.44       0.0000   \n",
       "50909  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50910  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50911  The Hunger Games                 11.43             0.22      -0.1027   \n",
       "50912  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50913  The Hunger Games                  2.86             0.06       0.0000   \n",
       "50914  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50915  The Hunger Games                  0.00             0.00      -0.5423   \n",
       "50916  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50917  The Hunger Games                  5.71             0.11       0.0000   \n",
       "50918  The Hunger Games                  0.00             0.00      -0.1027   \n",
       "50919  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50920  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50921  The Hunger Games                  0.00             0.00       0.0000   \n",
       "50922  The Hunger Games                  0.00             0.00       0.0000   \n",
       "\n",
       "       comp_pasive  comp_attribut  \n",
       "50907      -0.3612         0.0000  \n",
       "50908       0.0000         0.0000  \n",
       "50909       0.0000         0.0000  \n",
       "50910       0.0000         0.0000  \n",
       "50911       0.0000         0.0000  \n",
       "50912      -0.5423         0.0000  \n",
       "50913       0.0000         0.0000  \n",
       "50914       0.0000         0.0000  \n",
       "50915       0.0000         0.0000  \n",
       "50916       0.0000         0.0000  \n",
       "50917       0.0000         0.0000  \n",
       "50918      -0.6597        -0.5859  \n",
       "50919       0.0000         0.0000  \n",
       "50920       0.0000         0.0000  \n",
       "50921       0.0000         0.0000  \n",
       "50922       0.0000         0.0000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepared_copie[df_prepared_copie['wikiID']==31186339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d608aa70-0816-4e68-bbab-c660c68b0ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prepared_copie.to_csv('caracter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534c69c-8a09-42eb-a5e3-f2bdc3a490d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
